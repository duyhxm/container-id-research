# DVC Pipeline vs Standalone Files - Explanation

**Date**: December 10, 2024  
**Topic**: Understanding DVC data management in container-id-research project

---

## ğŸ¯ TL;DR

**Dataset `data/processed/detection` KHÃ”NG cÃ³ file `.dvc` riÃªng!**

NÃ³ Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi **DVC pipeline** trong `dvc.yaml`, hash Ä‘Æ°á»£c lÆ°u trong `dvc.lock`.

---

## ğŸ“š DVC cÃ³ 2 cÃ¡ch quáº£n lÃ½ data:

### **Method 1: Standalone Files** (`.dvc` files)

**CÃ¡ch dÃ¹ng:**
```bash
dvc add data/raw/
dvc add data/annotations/
```

**Káº¿t quáº£:**
```
data/
â”œâ”€â”€ raw.dvc           â† File tracking metadata
â”œâ”€â”€ annotations.dvc   â† File tracking metadata
â”œâ”€â”€ raw/              (actual data, gitignored)
â””â”€â”€ annotations/      (actual data, gitignored)
```

**Git tracks:**
- âœ… `data/raw.dvc`
- âœ… `data/annotations.dvc`
- âŒ NOT the actual data folders

**Push/Pull:**
```bash
dvc push data/raw.dvc
dvc pull data/raw.dvc
```

---

### **Method 2: Pipeline Outputs** (trong `dvc.yaml` + `dvc.lock`)

**CÃ¡ch Ä‘á»‹nh nghÄ©a:**

```yaml
# dvc.yaml
stages:
  convert_detection:
    cmd: python src/data/coco_to_yolo.py ...
    deps:
      - data/interim/train_master.json
      - src/data/coco_to_yolo.py
    outs:
      - data/processed/detection  â† Pipeline output!
```

**Káº¿t quáº£:**
```
data/processed/
â””â”€â”€ detection/        (actual data, gitignored)

# NO detection.dvc file created!
```

**Hash Ä‘Æ°á»£c lÆ°u trong `dvc.lock`:**
```yaml
# dvc.lock
stages:
  convert_detection:
    outs:
    - path: data/processed/detection
      hash: md5
      md5: 91b20250d3ea6dd41cca724079718820.dir
      size: 111035997
      nfiles: 1005
```

**Git tracks:**
- âœ… `dvc.yaml` (pipeline definition)
- âœ… `dvc.lock` (hash + metadata)
- âŒ NOT `data/processed/detection.dvc` (doesn't exist!)
- âŒ NOT the actual data folder

**Push/Pull:**
```bash
# Push ALL pipeline outputs
dvc push

# Pull ALL pipeline outputs  
dvc pull

# Or specific stage
dvc push -r storage convert_detection
dvc pull convert_detection
```

---

## ğŸ” Our Project Structure

### **Standalone `.dvc` files:**
```
data/raw.dvc              â†’ tracks data/raw/
data/annotations.dvc      â†’ tracks data/annotations/
```

### **Pipeline outputs** (NO `.dvc` files):
```
data/interim/
â”œâ”€â”€ augmented_images/     (from split_data stage)
â”œâ”€â”€ train_master.json     (from split_data stage)
â”œâ”€â”€ val_master.json       (from split_data stage)
â””â”€â”€ test_master.json      (from split_data stage)

data/processed/
â”œâ”€â”€ detection/            (from convert_detection stage)
â””â”€â”€ localization/         (from convert_localization stage)
```

**All tracked in `dvc.lock`, NOT separate `.dvc` files!**

---

## ğŸš¨ Common Mistake (What we fixed)

### **Wrong Code:**
```python
# Looking for detection.dvc (doesn't exist!)
if not os.path.exists("data/processed/detection.dvc"):
    print("âŒ DVC tracking file not found!")
    sys.exit(1)

# Trying to pull with .dvc file (wrong!)
os.system("dvc pull data/processed/detection.dvc")
```

### **Correct Code:**
```python
# Check dvc.lock (pipeline metadata)
if not os.path.exists("dvc.lock"):
    print("âŒ dvc.lock not found!")
    sys.exit(1)

# Pull all pipeline outputs
os.system("dvc pull")
```

---

## ğŸ“Š DVC Pipeline Visualization

```
data/annotations.dvc
        â†“
   [split_data]
        â†“
   data/interim/*
        â†“
 [convert_detection]
        â†“
data/processed/detection  â† NO .dvc file!
```

**See DAG:**
```bash
dvc dag
```

Output:
```
+----------------------+
| data\annotations.dvc |
+----------------------+
            *
            *
         +------------+
         | split_data |
         +------------+
        *             *
       *               *
+-------------------+  +----------------------+
| convert_detection |  | convert_localization |
+-------------------+  +----------------------+
```

---

## ğŸ”§ How to Work with Pipeline Outputs

### **Check Status:**
```bash
# Local vs lock file
dvc status

# Local vs remote (Google Drive)
dvc status -c
```

### **Push to Remote:**
```bash
# Push all pipeline outputs
dvc push

# Push specific stage output
dvc push -r storage convert_detection
```

### **Pull from Remote:**
```bash
# Pull all pipeline outputs
dvc pull

# Pull specific stage
dvc pull convert_detection
```

### **Re-run Pipeline:**
```bash
# Run specific stage
dvc repro convert_detection

# Run entire pipeline
dvc repro
```

### **Commit Changes:**
```bash
# After pipeline run, commit changes to outputs
dvc commit convert_detection

# Then push to remote
dvc push
```

---

## ğŸ’¡ Key Differences

| Aspect | Standalone `.dvc` | Pipeline Output |
|--------|-------------------|-----------------|
| **Created by** | `dvc add` | `dvc repro` |
| **Tracked in** | `.dvc` file | `dvc.lock` |
| **File exists** | âœ… Yes | âŒ No |
| **Git commits** | `.dvc` file | `dvc.lock` |
| **Push command** | `dvc push file.dvc` | `dvc push` |
| **Pull command** | `dvc pull file.dvc` | `dvc pull` |
| **Dependencies** | None | Defined in `dvc.yaml` |
| **Reproducible** | âŒ No | âœ… Yes (can `dvc repro`) |

---

## ğŸ“ When to Use Each Method

### **Use Standalone `.dvc` files when:**
- Raw data that never changes
- External datasets downloaded once
- No processing pipeline needed
- Example: `data/raw/`, `data/annotations/`

### **Use Pipeline Outputs when:**
- Data is generated by scripts
- Multiple processing stages
- Need reproducibility
- Want to track parameters
- Example: `data/processed/detection/`, `data/interim/*`

---

## âœ… Our Kaggle Training Fix

**Problem**: Code looked for `detection.dvc` (doesn't exist)

**Solution**: 
1. Check `dvc.lock` instead
2. Use `dvc pull` (all outputs) instead of `dvc pull detection.dvc`
3. Push all outputs: `dvc push` â†’ 3 files pushed
4. Verify: `dvc status -c` â†’ "Cache and remote in sync"

**Status**: âœ… Fixed in commit `ab18776`

---

## ğŸ”— References

- [DVC Pipeline Documentation](https://dvc.org/doc/user-guide/pipelines)
- [DVC Add vs Pipeline](https://dvc.org/doc/user-guide/pipelines/defining-pipelines#outputs-and-dependencies)
- Project files:
  - `dvc.yaml` - Pipeline definition
  - `dvc.lock` - Pipeline state (hashes, sizes)
  - `.dvc/config` - Remote configuration

---

## ğŸ“ Summary for Kaggle Training

**On Local Machine:**
```bash
# 1. Make sure pipeline outputs are pushed
dvc status -c

# If not in sync:
dvc push

# Verify
dvc status -c  # Should show: "Cache and remote in sync"
```

**On Kaggle Notebook:**
```python
# 1. Clone repo (includes dvc.lock)
git clone https://github.com/duyhxm/container-id-research.git

# 2. Configure DVC with service account
# (done in Step 4 of training notebook)

# 3. Pull ALL pipeline outputs
dvc pull  # â† NOT dvc pull detection.dvc!

# âœ… data/processed/detection/ will be fetched
```

**Key Insight**: Pipeline outputs are tracked collectively in `dvc.lock`, not individual `.dvc` files!

---

**Last Updated**: December 10, 2024  
**Maintainer**: Module 1 Team  
**Status**: Production-ready âœ…

