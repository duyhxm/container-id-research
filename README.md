# Container ID Extraction Research / NghiÃªn cá»©u TrÃ­ch xuáº¥t MÃ£ sá»‘ Container

[![Python Version](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![DVC](https://img.shields.io/badge/DVC-Enabled-brightgreen.svg)](https://dvc.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-In%20Development-yellow.svg)]()

**English** | [Tiáº¿ng Viá»‡t](#tiáº¿ng-viá»‡t)

---

## English

### Overview

This research project develops an end-to-end computer vision system for **automated container ID extraction** from images of **container doors**. The system is designed for SOWATCO company to streamline container identification in logistics operations.

In real-world deployment scenarios, users upload various images to the system. Before extracting container IDs, we must first detect and classify whether images contain container doors, assess image quality, localize the ID region, correct perspective distortion, and finally perform text extraction.

The project outputs trained models optimized for production deployment in a separate backend service.

### System Architecture

The solution implements a **5-stage modular pipeline**:

```mermaid
graph LR
    A[Input Image] --> B[Module 1: Door Detection]
    B --> C[Module 2: Quality Assessment]
    C --> D[Module 3: ID Localization]
    D --> E[Module 4: Perspective Correction]
    E --> F[Module 5: OCR Extraction]
    F --> G[Container ID Output]
```

#### Module Descriptions

1. **Module 1: Container Door Detection**
   - **Purpose**: Detect and classify container door objects in images
   - **Technology**: YOLOv11-Nano/Small
   - **Input**: Raw image
   - **Output**: Bounding box coordinates of container door

2. **Module 2: Image Quality Assessment**
   - **Purpose**: Evaluate image quality (blur, lighting, size)
   - **Technology**: Custom quality metrics / Lightweight classifier
   - **Input**: Cropped container door image
   - **Output**: Quality score and pass/fail decision

3. **Module 3: Container ID Localization**
   - **Purpose**: Predict 4-point polygon enclosing the container ID region
   - **Technology**: YOLOv11-Pose
   - **Input**: Container door image
   - **Output**: 4 keypoint coordinates (top-left, top-right, bottom-right, bottom-left)

4. **Module 4: Perspective Correction**
   - **Purpose**: Apply perspective transform to straighten the ID region
   - **Technology**: OpenCV perspective warp + quality assessment
   - **Input**: 4 keypoints from Module 3
   - **Output**: Rectified ID region image

5. **Module 5: OCR Extraction**
   - **Purpose**: Extract container ID text from rectified image
   - **Technology**: OCR engine (PaddleOCR / EasyOCR / Custom)
   - **Input**: Rectified ID image
   - **Output**: Container ID string (validated format)

### Technology Stack

- **Language**: Python 3.11
- **Deep Learning**: YOLOv11 (Ultralytics), PyTorch
- **Data Versioning**: DVC with Google Drive backend
- **Experiment Tracking**: Weights & Biases (wandb)
- **Dependency Management**: uv
- **Computer Vision**: OpenCV, Albumentations
- **Data Analysis**: Pandas, NumPy, Matplotlib, Seaborn

---

## Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Project Structure](#project-structure)
- [Installation & Setup](#installation--setup)
- [Data Management](#data-management)
- [Usage](#usage)
  - [Data Preparation](#data-preparation)
  - [Training Individual Modules](#training-individual-modules)
  - [Running Full Pipeline](#running-full-pipeline)
  - [Notebooks](#notebooks)
- [Experiment Tracking](#experiment-tracking)
- [Model Artifacts](#model-artifacts)
- [Development Guidelines](#development-guidelines)
- [Documentation](#documentation)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License & Acknowledgments](#license--acknowledgments)

---

## Project Structure

This repository follows a strict separation of concerns as defined in [.github/instructions/project_structure.instructions.md](.github/instructions/project_structure.instructions.md).

```text
container-id-research/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ instructions/       # Agent Rules (Single Source of Truth)
â”‚   â””â”€â”€ prompts/
â”‚
â”œâ”€â”€ artifacts/              # [GIT IGNORE] Automated Outputs
â”‚   â””â”€â”€ [module_name]/      # Organized by Module (e.g., detection, ocr)
â”‚       â””â”€â”€ [experiment_id]/
â”‚           â”œâ”€â”€ weights/
â”‚           â””â”€â”€ results.csv
â”‚
â”œâ”€â”€ data/                   # [DVC Managed]
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ [module_name]/  # Example: detection
â”‚           â”œâ”€â”€ images/
â”‚           â”œâ”€â”€ labels/
â”‚           â””â”€â”€ data.yaml   # <--- CRITICAL: Data config resides with data
â”‚
â”œâ”€â”€ demos/                  # Interactive Research Apps
â”‚   â””â”€â”€ [module_name]/      # Example: detection
â”‚       â”œâ”€â”€ app.py          # Entry point (Gradio/Streamlit)
â”‚       â””â”€â”€ samples/        # Test images specific to this demo
â”‚
â”œâ”€â”€ docs/                   # Documentation Center
â”‚   â”œâ”€â”€ guidelines/         # Labeling & SOPs
â”‚   â”œâ”€â”€ reports/            # Technical Reports
â”‚   â””â”€â”€ structure.md
â”‚
â”œâ”€â”€ experiments/            # [INPUT] Hyperparameter Configs
â”‚   â”œâ”€â”€ 001_det_baseline.yaml   # Naming: [id]_[module]_[description].yaml
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/              # Sandbox for EDA & Prototyping
â”‚   â”œâ”€â”€ 01_eda_detection.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                # Standalone Utility Scripts
â”‚   â”œâ”€â”€ kaggle/             # Remote training scripts
â”‚   â”œâ”€â”€ data_processing/    # One-off conversion scripts
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ src/                    # [LIBRARY] Reusable Core Logic (No execution code)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ [module_name]/      # Example: detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py        # Architecture definition
â”‚   â”‚   â”œâ”€â”€ dataset.py      # Custom Dataloader
â”‚   â”‚   â””â”€â”€ trainer.py      # Training loop logic
â”‚   â””â”€â”€ utils/              # Shared utilities (geometry, visualization)
â”‚
â”œâ”€â”€ tests/                  # Unit Tests mirroring src structure
â”œâ”€â”€ .env                    # Secrets (API Keys)
â”œâ”€â”€ dvc.yaml                # DVC Pipeline
â”œâ”€â”€ pyproject.toml          # Dependency Management
â””â”€â”€ README.md
```

### Directory Explanations

- **`src/` (The Library)**: Contains the core business logic and reusable components. Code here must be importable.
- **`experiments/` (The Configuration)**: Stores the "DNA" of every training run. A single YAML file must fully define an experiment.
- **`data/`**: All datasets managed by DVC. `raw/` contains original images, `interim/` holds stratified splits, `processed/` contains YOLO-formatted data.
- **`artifacts/` (The Output)**: Stores generated files (weights, logs, plots). This directory is git-ignored.
- **`scripts/` (The Executors)**: Entry points for execution. Scripts should import logic from `src` and configuration from `experiments`.
- **`notebooks/` (The Sandbox)**: Exploratory Data Analysis (EDA), prototyping, and visualization.
- **`demos/` (The Showcase)**: Interactive applications (Gradio/Streamlit) to demonstrate model capabilities.
- **`docs/`**: Documentation Center including guidelines, reports, and structure definitions.
- **`.github/`**: Contains agent instructions and prompts, serving as the single source of truth for project rules.

---

## Installation & Setup

### Prerequisites

- **Python 3.11** (required)
- **uv** (dependency manager)
- **Git** (version control)
- **DVC** (data version control)
- **Google Drive account** (for data access)

### Step-by-Step Installation

1. **Clone the repository**

```bash
git clone <repository-url>
cd container-id-research
```

2. **Install dependencies with uv**

```bash
# Install uv if not already installed
pip install uv

# Install project dependencies
uv sync
```

3. **Activate virtual environment**

```bash
# Windows
.venv\Scripts\activate

# Linux/macOS
source .venv/bin/activate
```

4. **Setup DVC and pull data**

```bash
# Initialize DVC (if needed)
dvc remote default storage

# Pull data from Google Drive
dvc pull
```

5. **Verify installation**

```bash
# Check Python version
python --version  # Should show 3.11.x

# Verify DVC data
ls data/raw/  # Should show images
```

---

## Data Management

### Dataset Overview

- **Total Images**: 831
- **Categories**: 2 (container_door, container_id)
- **Annotations**: 989 total (500 doors, 489 IDs)
- **Format**: COCO JSON for annotations, JPEG/PNG for images
- **Storage**: Google Drive via DVC

### Data Versioning with DVC

This project uses **DVC (Data Version Control)** to manage large datasets and model weights efficiently.

**Key DVC Files**:
- `data/raw.dvc`: Tracks the raw data directory
- `.dvc/config`: DVC remote configuration (Google Drive)
- `dvc.yaml`: DVC pipeline for data processing stages

**Common DVC Commands**:

```bash
# Pull latest data from remote
dvc pull

# Check data status
dvc status

# Reproduce entire pipeline
dvc repro

# Push new data to remote
dvc add data/raw
dvc push
```

### Data Stratification Methodology

The project employs **Label Powerset Stratification with Rare-Class Aggregation** to ensure balanced representation of edge cases in train/val/test splits.

**Key Features**:
- Stratification based on image attributes (lighting, angle, occlusion, surface, sharpness)
- Priority-based grouping (hard, tricky, common)
- Singleton handling with controlled augmentation
- Ratios: 70% Train, 15% Validation, 15% Test

ğŸ“š **Detailed methodology**: See [`docs/modules/module-1-detection/data-splitting-methodology.md`](docs/modules/module-1-detection/data-splitting-methodology.md)

---

## Usage

### Data Preparation

Run the DVC pipeline to process raw data and generate YOLO-formatted datasets:

```bash
# Execute full data pipeline
dvc repro

# Or run specific stages
dvc repro split_data
dvc repro convert_detection
dvc repro convert_localization
```

### Training Individual Modules

#### Module 1: Container Door Detection

**Local Training:**
```bash
# Train YOLOv11 detection model
python src/detection/train_and_evaluate.py --config experiments/001_det_baseline.yaml
```

**Kaggle Training (Recommended):**
> ğŸ“Œ For GPU-accelerated training on Kaggle (free T4/P100 GPUs), see: [**Kaggle Training Guide**](KAGGLE_TRAINING_GUIDE.md)
>
> The guide includes:
> - Direct notebook workflow (single-cell execution)
> - DVC session token setup (automatic model sync)
> - WandB integration for experiment tracking
> - Expected results: mAP@50 > 0.90, inference < 50ms

#### Module 3: Container ID Localization

```bash
# Train YOLOv11 pose model
python src/localization/train.py --config experiments/localization/exp001_baseline/config.yaml

# Run inference
python src/localization/inference.py --weights artifacts/localization/best.pt --source test_images/
```

### Running Full Pipeline

Execute the end-to-end pipeline on a single image or batch:

```bash
# Single image
python src/pipeline/full_pipeline.py --input path/to/image.jpg --output results/

# Batch processing
python src/pipeline/full_pipeline.py --input path/to/folder/ --output results/ --batch
```

### Notebooks

Explore data and visualize results using Jupyter notebooks:

```bash
# Launch Jupyter
jupyter notebook

# Available notebooks:
# - 01-annotated-image-eda.ipynb: Dataset exploration and statistics
# - 02-module1-detection-analysis.ipynb: Detection results analysis
# - 03-module3-localization-analysis.ipynb: Localization evaluation
# - 04-end-to-end-pipeline-demo.ipynb: Full pipeline demonstration
```

---

## Experiment Tracking

### Weights & Biases Integration

This project uses **wandb** for experiment tracking and visualization.

**Setup**:

```bash
# Login to wandb
wandb login

# Your experiments will be automatically logged
```

**Experiment Naming Convention**:

```
<module>_exp<number>_<description>

Examples:
- detection_exp001_yolo11n_baseline
- detection_exp002_yolo11s_augmented
- localization_exp001_yolo11_pose_baseline
```

**Tracked Metrics**:
- Training/validation loss curves
- mAP@50, mAP@50-95 (detection)
- OKS (Object Keypoint Similarity) for pose
- Inference time and FPS
- Model hyperparameters
- Dataset statistics

**Access Results**:
- Visit your wandb dashboard: `https://wandb.ai/<your-username>/container-id-research`
- Local results saved in `experiments/<module>/exp<number>/`

---

## Model Artifacts

### Model Storage

Trained models are stored in the `artifacts/` directory, organized by module:

```
artifacts/
â”œâ”€â”€ detection/
â”‚   â””â”€â”€ exp001/
â”‚       â”œâ”€â”€ weights/
â”‚       â”‚   â”œâ”€â”€ best.pt
â”‚       â”‚   â””â”€â”€ last.pt
â”‚       â””â”€â”€ results.csv
â””â”€â”€ localization/
    â””â”€â”€ exp001/
        â””â”€â”€ weights/
            â”œâ”€â”€ best.pt
            â””â”€â”€ last.pt
```

### Model Versioning Strategy

- **Git**: Track model metadata and configuration files
- **DVC**: Track large model weight files (`.pt` files)
- **wandb**: Track experiment history and model performance

### Exporting for Production

```bash
# Export models with metadata
python scripts/export_models.py --module detection --version v1.0

# Output: Packaged model ready for backend integration
```

### Performance Benchmarks

_(To be updated as training progresses)_

| Module          | Model        | mAP@50 | mAP@50-95 | Inference Time | Size |
| --------------- | ------------ | ------ | --------- | -------------- | ---- |
| Door Detection  | YOLOv11n     | TBD    | TBD       | TBD            | TBD  |
| ID Localization | YOLOv11-Pose | TBD    | TBD       | TBD            | TBD  |

---

## Development Guidelines

### Code Style

- **PEP 8**: Follow Python style guidelines
- **Black**: Code formatter (line length: 88)
- **isort**: Import sorting
- **Type hints**: Use type annotations where applicable

```bash
# Format code
black src/
isort src/

# Check style
flake8 src/
```

### Conventional Commits

This project follows **Unified Conventional Commits Standard (UCCS)**.

**Commit format**:
```
<type>(<scope>): <description>

[optional body]
```

**Types**:
- `feat`: New feature or capability
- `fix`: Bug fix or correction
- `refactor`: Code restructuring without behavior change
- `docs`: Documentation updates
- `style`: Code formatting (no logic change)
- `chore`: Maintenance tasks (dependencies, configs)

ğŸ“š **Full guidelines**: [`docs/general/conventional-commit-guideline.md`](docs/general/conventional-commit-guideline.md)

### Branch Strategy

- `main`: Production-ready code
- `develop`: Integration branch
- `feature/*`: New features
- `fix/*`: Bug fixes
- `experiment/*`: Experimental work

### Pull Request Workflow

1. Create feature branch from `develop`
2. Make changes with conventional commits
3. Write/update tests
4. Submit PR with clear description
5. Pass CI checks and code review
6. Merge to `develop`

### Testing Requirements

```bash
# Run all tests
pytest tests/

# Run specific module tests
pytest tests/test_detection.py

# Generate coverage report
pytest --cov=src tests/
```

---

## Documentation

### Available Documentation

#### General
- [Conventional Commit Guidelines](docs/general/conventional-commit-guideline.md)
- [System Architecture](docs/general/architecture.md) _(To be created)_

#### Data Labeling
- [Attribute Annotation Guideline](docs/data-labeling/attribute-annotation-guideline.md)
- [Container Door Labeling Guideline](docs/data-labeling/container-door-labeling-guideline.md)
- [Container ID Labeling Guideline](docs/data-labeling/id-container-labeling-guideline.md)

#### Module-Specific
- **Module 1 (Detection)**:
  - [Data Splitting Methodology](docs/modules/module-1-detection/data-splitting-methodology.md)
  - [Technical Specification: Data Splitting](docs/modules/module-1-detection/technical-specification-data-splitting.md)
  - [Training Guide](docs/modules/module-1-detection/training-guide.md) _(To be created)_

---

## Roadmap

### Current Status

- âœ… Data collection and annotation (831 images)
- âœ… Data labeling guidelines established
- âœ… Data stratification methodology designed
- âœ… EDA and dataset analysis complete
- ğŸŸ¡ Module 1 (Detection) - In development
- ğŸŸ¡ Module 3 (Localization) - In development

### Upcoming Milestones

- [ ] Complete Module 1 training and evaluation
- [ ] Complete Module 3 training and evaluation
- [ ] Implement Module 2 (Quality Assessment)
- [ ] Implement Module 4 (Perspective Correction)
- [ ] Implement Module 5 (OCR Extraction)
- [ ] End-to-end pipeline integration and testing
- [ ] Performance optimization and benchmarking
- [ ] Production deployment preparation
- [ ] Backend service integration

### Future Enhancements

- Real-time video processing
- Multi-container detection in single image
- Mobile deployment (TensorFlow Lite / ONNX)
- API service with FastAPI
- Continuous learning pipeline

---

## Contributing

### How to Contribute

We welcome contributions! Here's how you can help:

1. **Report Issues**: Use GitHub Issues for bug reports and feature requests
2. **Submit Pull Requests**: Follow the PR workflow outlined above
3. **Improve Documentation**: Fix typos, add examples, clarify explanations
4. **Share Ideas**: Discuss improvements in GitHub Discussions

### Contact

- **Project Lead**: duyhxm
- **Organization**: SOWATCO Company
- **Email**: _(Add if applicable)_

---

## License & Acknowledgments

### License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) file for details.

### Acknowledgments

- **SOWATCO Company** for project sponsorship and domain expertise
- **Ultralytics** for the YOLOv11 framework
- **DVC Team** for data versioning tools
- **Weights & Biases** for experiment tracking platform
- **CVAT** for annotation tools
- Open-source community for libraries: OpenCV, PyTorch, Albumentations

### References

- [YOLOv11 Documentation](https://docs.ultralytics.com/)
- [DVC Documentation](https://dvc.org/doc)
- [Weights & Biases Documentation](https://docs.wandb.ai/)
- [COCO Dataset Format](https://cocodataset.org/#format-data)

---

# Tiáº¿ng Viá»‡t

## Tá»•ng quan

Dá»± Ã¡n nghiÃªn cá»©u nÃ y phÃ¡t triá»ƒn má»™t há»‡ thá»‘ng thá»‹ giÃ¡c mÃ¡y tÃ­nh end-to-end Ä‘á»ƒ **tá»± Ä‘á»™ng trÃ­ch xuáº¥t mÃ£ sá»‘ container** tá»« hÃ¬nh áº£nh **cá»­a sau container**. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ´ng ty SOWATCO nháº±m tá»‘i Æ°u hÃ³a quy trÃ¬nh nháº­n diá»‡n container trong hoáº¡t Ä‘á»™ng logistics.

Trong mÃ´i trÆ°á»ng triá»ƒn khai thá»±c táº¿, ngÆ°á»i dÃ¹ng táº£i lÃªn nhiá»u loáº¡i hÃ¬nh áº£nh khÃ¡c nhau. TrÆ°á»›c khi trÃ­ch xuáº¥t mÃ£ sá»‘ container, chÃºng ta pháº£i phÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i xem áº£nh cÃ³ chá»©a cá»­a container khÃ´ng, Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng áº£nh, Ä‘á»‹nh vá»‹ vÃ¹ng chá»©a mÃ£ sá»‘, náº¯n chá»‰nh phá»‘i cáº£nh, vÃ  cuá»‘i cÃ¹ng thá»±c hiá»‡n trÃ­ch xuáº¥t vÄƒn báº£n.

Sáº£n pháº©m Ä‘áº§u ra cá»§a dá»± Ã¡n lÃ  cÃ¡c model Ä‘Æ°á»£c huáº¥n luyá»‡n tá»‘i Æ°u Ä‘á»ƒ triá»ƒn khai trong má»™t dá»‹ch vá»¥ backend riÃªng biá»‡t.

## Kiáº¿n trÃºc Há»‡ thá»‘ng

Giáº£i phÃ¡p triá»ƒn khai **pipeline 5 giai Ä‘oáº¡n modular**:

```mermaid
graph LR
    A[áº¢nh Ä‘áº§u vÃ o] --> B[Module 1: PhÃ¡t hiá»‡n Cá»­a]
    B --> C[Module 2: ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng]
    C --> D[Module 3: Äá»‹nh vá»‹ ID]
    D --> E[Module 4: Náº¯n chá»‰nh Phá»‘i cáº£nh]
    E --> F[Module 5: TrÃ­ch xuáº¥t OCR]
    F --> G[Káº¿t quáº£ MÃ£ Container]
```

### MÃ´ táº£ cÃ¡c Module

1. **Module 1: PhÃ¡t hiá»‡n Cá»­a Container**
   - **Má»¥c Ä‘Ã­ch**: PhÃ¡t hiá»‡n vÃ  phÃ¢n loáº¡i Ä‘á»‘i tÆ°á»£ng cá»­a container trong áº£nh
   - **CÃ´ng nghá»‡**: YOLOv11-Nano/Small
   - **Äáº§u vÃ o**: áº¢nh gá»‘c
   - **Äáº§u ra**: Tá»a Ä‘á»™ bounding box cá»§a cá»­a container

2. **Module 2: ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng áº¢nh**
   - **Má»¥c Ä‘Ã­ch**: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng áº£nh (Ä‘á»™ má», Ã¡nh sÃ¡ng, kÃ­ch thÆ°á»›c)
   - **CÃ´ng nghá»‡**: Chá»‰ sá»‘ cháº¥t lÆ°á»£ng tÃ¹y chá»‰nh / Classifier nháº¹
   - **Äáº§u vÃ o**: áº¢nh cá»­a container Ä‘Ã£ crop
   - **Äáº§u ra**: Äiá»ƒm cháº¥t lÆ°á»£ng vÃ  quyáº¿t Ä‘á»‹nh pass/fail

3. **Module 3: Äá»‹nh vá»‹ MÃ£ sá»‘ Container**
   - **Má»¥c Ä‘Ã­ch**: Dá»± Ä‘oÃ¡n Ä‘a giÃ¡c 4 Ä‘iá»ƒm bao quanh vÃ¹ng chá»©a mÃ£ container
   - **CÃ´ng nghá»‡**: YOLOv11-Pose
   - **Äáº§u vÃ o**: áº¢nh cá»­a container
   - **Äáº§u ra**: 4 tá»a Ä‘á»™ keypoint (trÃªn-trÃ¡i, trÃªn-pháº£i, dÆ°á»›i-pháº£i, dÆ°á»›i-trÃ¡i)

4. **Module 4: Náº¯n chá»‰nh Phá»‘i cáº£nh**
   - **Má»¥c Ä‘Ã­ch**: Ãp dá»¥ng biáº¿n Ä‘á»•i phá»‘i cáº£nh Ä‘á»ƒ lÃ m tháº³ng vÃ¹ng ID
   - **CÃ´ng nghá»‡**: OpenCV perspective warp + Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng
   - **Äáº§u vÃ o**: 4 keypoint tá»« Module 3
   - **Äáº§u ra**: áº¢nh vÃ¹ng ID Ä‘Ã£ Ä‘Æ°á»£c náº¯n chá»‰nh

5. **Module 5: TrÃ­ch xuáº¥t OCR**
   - **Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t vÄƒn báº£n mÃ£ container tá»« áº£nh Ä‘Ã£ náº¯n chá»‰nh
   - **CÃ´ng nghá»‡**: OCR engine (PaddleOCR / EasyOCR / Custom)
   - **Äáº§u vÃ o**: áº¢nh ID Ä‘Ã£ náº¯n chá»‰nh
   - **Äáº§u ra**: Chuá»—i mÃ£ container (Ä‘á»‹nh dáº¡ng Ä‘Ã£ validate)

## NgÄƒn xáº¿p CÃ´ng nghá»‡

- **NgÃ´n ngá»¯**: Python 3.11
- **Deep Learning**: YOLOv11 (Ultralytics), PyTorch
- **Quáº£n lÃ½ phiÃªn báº£n dá»¯ liá»‡u**: DVC vá»›i Google Drive backend
- **Theo dÃµi thá»±c nghiá»‡m**: Weights & Biases (wandb)
- **Quáº£n lÃ½ dependencies**: uv
- **Computer Vision**: OpenCV, Albumentations
- **PhÃ¢n tÃ­ch dá»¯ liá»‡u**: Pandas, NumPy, Matplotlib, Seaborn

---

## Má»¥c lá»¥c

- [Tá»•ng quan](#tá»•ng-quan)
- [Kiáº¿n trÃºc Há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [Cáº¥u trÃºc Dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Ä‘áº·t & Thiáº¿t láº­p](#cÃ i-Ä‘áº·t--thiáº¿t-láº­p)
- [Quáº£n lÃ½ Dá»¯ liá»‡u](#quáº£n-lÃ½-dá»¯-liá»‡u)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [Theo dÃµi Thá»±c nghiá»‡m](#theo-dÃµi-thá»±c-nghiá»‡m)
- [Sáº£n pháº©m Model](#sáº£n-pháº©m-model)
- [HÆ°á»›ng dáº«n PhÃ¡t triá»ƒn](#hÆ°á»›ng-dáº«n-phÃ¡t-triá»ƒn)
- [TÃ i liá»‡u](#tÃ i-liá»‡u)
- [Lá»™ trÃ¬nh](#lá»™-trÃ¬nh)
- [ÄÃ³ng gÃ³p](#Ä‘Ã³ng-gÃ³p)
- [Giáº¥y phÃ©p & Ghi nháº­n](#giáº¥y-phÃ©p--ghi-nháº­n)

---

## Cáº¥u trÃºc Dá»± Ã¡n

Dá»± Ã¡n nÃ y tuÃ¢n thá»§ nghiÃªm ngáº·t viá»‡c phÃ¢n tÃ¡ch cÃ¡c má»‘i quan tÃ¢m nhÆ° Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong [.github/instructions/project_structure.instructions.md](.github/instructions/project_structure.instructions.md).

```text
container-id-research/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ instructions/       # Quy táº¯c Agent (Nguá»“n sá»± tháº­t duy nháº¥t)
â”‚   â””â”€â”€ prompts/
â”‚
â”œâ”€â”€ artifacts/              # [GIT IGNORE] Äáº§u ra tá»± Ä‘á»™ng
â”‚   â””â”€â”€ [module_name]/      # Tá»• chá»©c theo Module (vd: detection, ocr)
â”‚       â””â”€â”€ [experiment_id]/
â”‚           â”œâ”€â”€ weights/
â”‚           â””â”€â”€ results.csv
â”‚
â”œâ”€â”€ data/                   # [DVC Managed]
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ [module_name]/  # Vd: detection
â”‚           â”œâ”€â”€ images/
â”‚           â”œâ”€â”€ labels/
â”‚           â””â”€â”€ data.yaml   # <--- QUAN TRá»ŒNG: Config dá»¯ liá»‡u náº±m cÃ¹ng dá»¯ liá»‡u
â”‚
â”œâ”€â”€ demos/                  # á»¨ng dá»¥ng nghiÃªn cá»©u tÆ°Æ¡ng tÃ¡c
â”‚   â””â”€â”€ [module_name]/      # Vd: detection
â”‚       â”œâ”€â”€ app.py          # Äiá»ƒm nháº­p (Gradio/Streamlit)
â”‚       â””â”€â”€ samples/        # áº¢nh test cá»¥ thá»ƒ cho demo nÃ y
â”‚
â”œâ”€â”€ docs/                   # Trung tÃ¢m TÃ i liá»‡u
â”‚   â”œâ”€â”€ guidelines/         # HÆ°á»›ng dáº«n gÃ¡n nhÃ£n & SOPs
â”‚   â”œâ”€â”€ reports/            # BÃ¡o cÃ¡o ká»¹ thuáº­t
â”‚   â””â”€â”€ structure.md
â”‚
â”œâ”€â”€ experiments/            # [INPUT] Cáº¥u hÃ¬nh Hyperparameter
â”‚   â”œâ”€â”€ 001_det_baseline.yaml   # Äáº·t tÃªn: [id]_[module]_[description].yaml
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/              # Sandbox cho EDA & Prototyping
â”‚   â”œâ”€â”€ 01_eda_detection.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                # Scripts tiá»‡n Ã­ch Ä‘á»™c láº­p
â”‚   â”œâ”€â”€ kaggle/             # Scripts training tá»« xa
â”‚   â”œâ”€â”€ data_processing/    # Scripts chuyá»ƒn Ä‘á»•i má»™t láº§n
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ src/                    # [THÆ¯ VIá»†N] Logic cá»‘t lÃµi tÃ¡i sá»­ dá»¥ng (KhÃ´ng code thá»±c thi)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ [module_name]/      # Vd: detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py        # Äá»‹nh nghÄ©a kiáº¿n trÃºc
â”‚   â”‚   â”œâ”€â”€ dataset.py      # Custom Dataloader
â”‚   â”‚   â””â”€â”€ trainer.py      # Logic vÃ²ng láº·p training
â”‚   â””â”€â”€ utils/              # Tiá»‡n Ã­ch chia sáº» (hÃ¬nh há»c, trá»±c quan hÃ³a)
â”‚
â”œâ”€â”€ tests/                  # Unit Tests pháº£n chiáº¿u cáº¥u trÃºc src
â”œâ”€â”€ .env                    # Secrets (API Keys)
â”œâ”€â”€ dvc.yaml                # DVC Pipeline
â”œâ”€â”€ pyproject.toml          # Quáº£n lÃ½ Dependency
â””â”€â”€ README.md
```

### Giáº£i thÃ­ch ThÆ° má»¥c

- **`src/` (ThÆ° viá»‡n)**: Chá»©a logic nghiá»‡p vá»¥ cá»‘t lÃµi vÃ  cÃ¡c thÃ nh pháº§n tÃ¡i sá»­ dá»¥ng. Code á»Ÿ Ä‘Ã¢y pháº£i cÃ³ thá»ƒ import Ä‘Æ°á»£c.
- **`experiments/` (Cáº¥u hÃ¬nh)**: LÆ°u trá»¯ "DNA" cá»§a má»—i láº§n cháº¡y training. Má»™t file YAML duy nháº¥t pháº£i Ä‘á»‹nh nghÄ©a Ä‘áº§y Ä‘á»§ má»™t thá»±c nghiá»‡m.
- **`data/`**: Táº¥t cáº£ datasets Ä‘Æ°á»£c quáº£n lÃ½ bá»Ÿi DVC. `raw/` chá»©a áº£nh gá»‘c, `interim/` chá»©a cÃ¡c split Ä‘Ã£ phÃ¢n táº§ng, `processed/` chá»©a dá»¯ liá»‡u Ä‘á»‹nh dáº¡ng YOLO.
- **`artifacts/` (Äáº§u ra)**: LÆ°u trá»¯ cÃ¡c file Ä‘Æ°á»£c táº¡o ra (trá»ng sá»‘, logs, biá»ƒu Ä‘á»“). ThÆ° má»¥c nÃ y Ä‘Æ°á»£c git-ignore.
- **`scripts/` (NgÆ°á»i thá»±c thi)**: Äiá»ƒm nháº­p Ä‘á»ƒ thá»±c thi. Scripts nÃªn import logic tá»« `src` vÃ  cáº¥u hÃ¬nh tá»« `experiments`.
- **`notebooks/` (Sandbox)**: PhÃ¢n tÃ­ch dá»¯ liá»‡u khÃ¡m phÃ¡ (EDA), táº¡o máº«u vÃ  trá»±c quan hÃ³a.
- **`demos/` (TrÆ°ng bÃ y)**: CÃ¡c á»©ng dá»¥ng tÆ°Æ¡ng tÃ¡c (Gradio/Streamlit) Ä‘á»ƒ demo kháº£ nÄƒng cá»§a model.
- **`docs/`**: Trung tÃ¢m tÃ i liá»‡u bao gá»“m hÆ°á»›ng dáº«n, bÃ¡o cÃ¡o vÃ  Ä‘á»‹nh nghÄ©a cáº¥u trÃºc.
- **`.github/`**: Chá»©a hÆ°á»›ng dáº«n vÃ  prompts cho agent, Ä‘Ã³ng vai trÃ² lÃ  nguá»“n sá»± tháº­t duy nháº¥t cho cÃ¡c quy táº¯c dá»± Ã¡n.

---

## CÃ i Ä‘áº·t & Thiáº¿t láº­p

### YÃªu cáº§u

- **Python 3.11** (báº¯t buá»™c)
- **uv** (quáº£n lÃ½ dependencies)
- **Git** (version control)
- **DVC** (data version control)
- **TÃ i khoáº£n Google Drive** (Ä‘á»ƒ truy cáº­p dá»¯ liá»‡u)

### HÆ°á»›ng dáº«n CÃ i Ä‘áº·t Chi tiáº¿t

1. **Clone repository**

```bash
git clone <repository-url>
cd container-id-research
```

2. **CÃ i Ä‘áº·t dependencies vá»›i uv**

```bash
# CÃ i Ä‘áº·t uv náº¿u chÆ°a cÃ³
pip install uv

# CÃ i Ä‘áº·t dependencies cá»§a dá»± Ã¡n
uv sync
```

3. **KÃ­ch hoáº¡t mÃ´i trÆ°á»ng áº£o**

```bash
# Windows
.venv\Scripts\activate

# Linux/macOS
source .venv/bin/activate
```

4. **Thiáº¿t láº­p DVC vÃ  táº£i dá»¯ liá»‡u**

```bash
# Khá»Ÿi táº¡o DVC (náº¿u cáº§n)
dvc remote default storage

# Táº£i dá»¯ liá»‡u tá»« Google Drive
dvc pull
```

5. **Kiá»ƒm tra cÃ i Ä‘áº·t**

```bash
# Kiá»ƒm tra phiÃªn báº£n Python
python --version  # Pháº£i hiá»ƒn thá»‹ 3.11.x

# Kiá»ƒm tra dá»¯ liá»‡u DVC
ls data/raw/  # Pháº£i hiá»ƒn thá»‹ áº£nh
```

---

## Quáº£n lÃ½ Dá»¯ liá»‡u

### Tá»•ng quan Dataset

- **Tá»•ng sá»‘ áº£nh**: 831
- **Danh má»¥c**: 2 (container_door, container_id)
- **Annotations**: 989 tá»•ng (500 cá»­a, 489 ID)
- **Äá»‹nh dáº¡ng**: COCO JSON cho annotations, JPEG/PNG cho áº£nh
- **LÆ°u trá»¯**: Google Drive qua DVC

### Quáº£n lÃ½ phiÃªn báº£n Dá»¯ liá»‡u vá»›i DVC

Dá»± Ã¡n nÃ y sá»­ dá»¥ng **DVC (Data Version Control)** Ä‘á»ƒ quáº£n lÃ½ datasets lá»›n vÃ  trá»ng sá»‘ model hiá»‡u quáº£.

**CÃ¡c file DVC quan trá»ng**:
- `data/raw.dvc`: Theo dÃµi thÆ° má»¥c dá»¯ liá»‡u gá»‘c
- `.dvc/config`: Cáº¥u hÃ¬nh DVC remote (Google Drive)
- `dvc.yaml`: DVC pipeline cho cÃ¡c giai Ä‘oáº¡n xá»­ lÃ½ dá»¯ liá»‡u

**Lá»‡nh DVC thÃ´ng dá»¥ng**:

```bash
# Táº£i dá»¯ liá»‡u má»›i nháº¥t tá»« remote
dvc pull

# Kiá»ƒm tra tráº¡ng thÃ¡i dá»¯ liá»‡u
dvc status

# TÃ¡i táº¡o toÃ n bá»™ pipeline
dvc repro

# Äáº©y dá»¯ liá»‡u má»›i lÃªn remote
dvc add data/raw
dvc push
```

### PhÆ°Æ¡ng phÃ¡p PhÃ¢n táº§ng Dá»¯ liá»‡u

Dá»± Ã¡n Ã¡p dá»¥ng **PhÃ¢n táº§ng dá»±a trÃªn Label Powerset vá»›i Gá»™p nhÃ³m Rare-Class** Ä‘á»ƒ Ä‘áº£m báº£o Ä‘áº¡i diá»‡n cÃ¢n báº±ng cÃ¡c trÆ°á»ng há»£p biÃªn trong train/val/test splits.

**Äáº·c Ä‘iá»ƒm chÃ­nh**:
- PhÃ¢n táº§ng dá»±a trÃªn thuá»™c tÃ­nh áº£nh (Ã¡nh sÃ¡ng, gÃ³c, che khuáº¥t, bá» máº·t, Ä‘á»™ nÃ©t)
- Gom nhÃ³m theo Æ°u tiÃªn (hard, tricky, common)
- Xá»­ lÃ½ singleton vá»›i augmentation cÃ³ kiá»ƒm soÃ¡t
- Tá»· lá»‡: 70% Train, 15% Validation, 15% Test

ğŸ“š **PhÆ°Æ¡ng phÃ¡p chi tiáº¿t**: Xem [`docs/modules/module-1-detection/data-splitting-methodology.md`](docs/modules/module-1-detection/data-splitting-methodology.md)

---

## Sá»­ dá»¥ng

### Chuáº©n bá»‹ Dá»¯ liá»‡u

Cháº¡y DVC pipeline Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u gá»‘c vÃ  táº¡o datasets Ä‘á»‹nh dáº¡ng YOLO:

```bash
# Thá»±c thi toÃ n bá»™ data pipeline
dvc repro

# Hoáº·c cháº¡y cÃ¡c giai Ä‘oáº¡n cá»¥ thá»ƒ
dvc repro split_data
dvc repro convert_detection
dvc repro convert_localization
```

### Huáº¥n luyá»‡n cÃ¡c Module RiÃªng láº»

#### Module 1: PhÃ¡t hiá»‡n Cá»­a Container

```bash
# Huáº¥n luyá»‡n YOLOv11 detection model
python src/detection/train_and_evaluate.py --config experiments/001_det_baseline.yaml
```

#### Module 3: Äá»‹nh vá»‹ MÃ£ sá»‘ Container

```bash
# Huáº¥n luyá»‡n YOLOv11 pose model
python src/localization/train.py --config experiments/localization/exp001_baseline/config.yaml

# Cháº¡y inference
python src/localization/inference.py --weights artifacts/localization/best.pt --source test_images/
```

### Cháº¡y Pipeline Äáº§y Ä‘á»§

Thá»±c thi pipeline end-to-end trÃªn má»™t áº£nh hoáº·c batch:

```bash
# Má»™t áº£nh
python src/pipeline/full_pipeline.py --input path/to/image.jpg --output results/

# Xá»­ lÃ½ batch
python src/pipeline/full_pipeline.py --input path/to/folder/ --output results/ --batch
```

### Notebooks

KhÃ¡m phÃ¡ dá»¯ liá»‡u vÃ  trá»±c quan hÃ³a káº¿t quáº£ báº±ng Jupyter notebooks:

```bash
# Khá»Ÿi Ä‘á»™ng Jupyter
jupyter notebook

# Notebooks cÃ³ sáºµn:
# - 01-annotated-image-eda.ipynb: KhÃ¡m phÃ¡ dataset vÃ  thá»‘ng kÃª
# - 02-module1-detection-analysis.ipynb: PhÃ¢n tÃ­ch káº¿t quáº£ detection
# - 03-module3-localization-analysis.ipynb: ÄÃ¡nh giÃ¡ localization
# - 04-end-to-end-pipeline-demo.ipynb: Demo pipeline Ä‘áº§y Ä‘á»§
```

---

## Theo dÃµi Thá»±c nghiá»‡m

### TÃ­ch há»£p Weights & Biases

Dá»± Ã¡n nÃ y sá»­ dá»¥ng **wandb** Ä‘á»ƒ theo dÃµi vÃ  trá»±c quan hÃ³a thá»±c nghiá»‡m.

**Thiáº¿t láº­p**:

```bash
# ÄÄƒng nháº­p wandb
wandb login

# CÃ¡c thá»±c nghiá»‡m sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c log
```

**Quy Æ°á»›c Äáº·t tÃªn Thá»±c nghiá»‡m**:

```
<module>_exp<sá»‘>_<mÃ´_táº£>

VÃ­ dá»¥:
- detection_exp001_yolo11n_baseline
- detection_exp002_yolo11s_augmented
- localization_exp001_yolo11_pose_baseline
```

**Chá»‰ sá»‘ Ä‘Æ°á»£c Theo dÃµi**:
- ÄÆ°á»ng cong loss training/validation
- mAP@50, mAP@50-95 (detection)
- OKS (Object Keypoint Similarity) cho pose
- Thá»i gian inference vÃ  FPS
- Hyperparameters cá»§a model
- Thá»‘ng kÃª dataset

**Truy cáº­p Káº¿t quáº£**:
- Truy cáº­p wandb dashboard: `https://wandb.ai/<tÃªn-ngÆ°á»i-dÃ¹ng>/container-id-research`
- Káº¿t quáº£ local Ä‘Æ°á»£c lÆ°u trong `experiments/<module>/exp<sá»‘>/`

---

## Sáº£n pháº©m Model

### LÆ°u trá»¯ Model

CÃ¡c model Ä‘Ã£ train Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `artifacts/`, tá»• chá»©c theo module:

```
artifacts/
â”œâ”€â”€ detection/
â”‚   â””â”€â”€ exp001/
â”‚       â”œâ”€â”€ weights/
â”‚       â”‚   â”œâ”€â”€ best.pt
â”‚       â”‚   â””â”€â”€ last.pt
â”‚       â””â”€â”€ results.csv
â””â”€â”€ localization/
    â””â”€â”€ exp001/
        â””â”€â”€ weights/
            â”œâ”€â”€ best.pt
            â””â”€â”€ last.pt
```

### Chiáº¿n lÆ°á»£c PhiÃªn báº£n Model

- **Git**: Theo dÃµi metadata vÃ  file cáº¥u hÃ¬nh model
- **DVC**: Theo dÃµi file trá»ng sá»‘ model lá»›n (`.pt` files)
- **wandb**: Theo dÃµi lá»‹ch sá»­ thá»±c nghiá»‡m vÃ  hiá»‡u nÄƒng model

### Xuáº¥t cho Production

```bash
# Xuáº¥t models vá»›i metadata
python scripts/export_models.py --module detection --version v1.0

# Output: Model Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i sáºµn sÃ ng cho tÃ­ch há»£p backend
```

### Benchmark Hiá»‡u nÄƒng

_(Sáº½ Ä‘Æ°á»£c cáº­p nháº­t khi quÃ¡ trÃ¬nh training hoÃ n thÃ nh)_

| Module        | Model        | mAP@50 | mAP@50-95 | Thá»i gian Inference | KÃ­ch thÆ°á»›c |
| ------------- | ------------ | ------ | --------- | ------------------- | ---------- |
| PhÃ¡t hiá»‡n Cá»­a | YOLOv11n     | TBD    | TBD       | TBD                 | TBD        |
| Äá»‹nh vá»‹ ID    | YOLOv11-Pose | TBD    | TBD       | TBD                 | TBD        |

---

## HÆ°á»›ng dáº«n PhÃ¡t triá»ƒn

### Phong cÃ¡ch Code

- **PEP 8**: TuÃ¢n thá»§ hÆ°á»›ng dáº«n style Python
- **Black**: Code formatter (Ä‘á»™ dÃ i dÃ²ng: 88)
- **isort**: Sáº¯p xáº¿p import
- **Type hints**: Sá»­ dá»¥ng type annotations khi cÃ³ thá»ƒ

```bash
# Format code
black src/
isort src/

# Kiá»ƒm tra style
flake8 src/
```

### Conventional Commits

Dá»± Ã¡n nÃ y tuÃ¢n theo **Unified Conventional Commits Standard (UCCS)**.

**Äá»‹nh dáº¡ng commit**:
```
<type>(<scope>): <description>

[optional body]
```

**Types**:
- `feat`: TÃ­nh nÄƒng hoáº·c kháº£ nÄƒng má»›i
- `fix`: Sá»­a lá»—i hoáº·c hiá»‡u chá»‰nh
- `refactor`: TÃ¡i cáº¥u trÃºc code khÃ´ng thay Ä‘á»•i hÃ nh vi
- `docs`: Cáº­p nháº­t tÃ i liá»‡u
- `style`: Format code (khÃ´ng thay Ä‘á»•i logic)
- `chore`: TÃ¡c vá»¥ báº£o trÃ¬ (dependencies, configs)

ğŸ“š **HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§**: [`docs/general/conventional-commit-guideline.md`](docs/general/conventional-commit-guideline.md)

### Chiáº¿n lÆ°á»£c Branch

- `main`: Code sáºµn sÃ ng production
- `develop`: Branch tÃ­ch há»£p
- `feature/*`: TÃ­nh nÄƒng má»›i
- `fix/*`: Sá»­a lá»—i
- `experiment/*`: CÃ´ng viá»‡c thá»­ nghiá»‡m

### Quy trÃ¬nh Pull Request

1. Táº¡o feature branch tá»« `develop`
2. Thá»±c hiá»‡n thay Ä‘á»•i vá»›i conventional commits
3. Viáº¿t/cáº­p nháº­t tests
4. Submit PR vá»›i mÃ´ táº£ rÃµ rÃ ng
5. Pass CI checks vÃ  code review
6. Merge vÃ o `develop`

### YÃªu cáº§u Testing

```bash
# Cháº¡y táº¥t cáº£ tests
pytest tests/

# Cháº¡y tests module cá»¥ thá»ƒ
pytest tests/test_detection.py

# Táº¡o bÃ¡o cÃ¡o coverage
pytest --cov=src tests/
```

---

## TÃ i liá»‡u

### TÃ i liá»‡u CÃ³ sáºµn

#### Chung
- [HÆ°á»›ng dáº«n Conventional Commit](docs/general/conventional-commit-guideline.md)
- [Kiáº¿n trÃºc Há»‡ thá»‘ng](docs/general/architecture.md) _(Sáº½ táº¡o)_

#### GÃ¡n nhÃ£n Dá»¯ liá»‡u
- [HÆ°á»›ng dáº«n GÃ¡n nhÃ£n Thuá»™c tÃ­nh](docs/data-labeling/attribute-annotation-guideline.md)
- [HÆ°á»›ng dáº«n GÃ¡n nhÃ£n Cá»­a Container](docs/data-labeling/container-door-labeling-guideline.md)
- [HÆ°á»›ng dáº«n GÃ¡n nhÃ£n ID Container](docs/data-labeling/id-container-labeling-guideline.md)

#### Theo Module
- **Module 1 (Detection)**:
  - [PhÆ°Æ¡ng phÃ¡p PhÃ¢n táº§ng Dá»¯ liá»‡u](docs/modules/module-1-detection/data-splitting-methodology.md)
  - [Äáº·c táº£ Ká»¹ thuáº­t: PhÃ¢n táº§ng Dá»¯ liá»‡u](docs/modules/module-1-detection/technical-specification-data-splitting.md)
  - [HÆ°á»›ng dáº«n Training](docs/modules/module-1-detection/training-guide.md) _(Sáº½ táº¡o)_

---

## Lá»™ trÃ¬nh

### Tráº¡ng thÃ¡i Hiá»‡n táº¡i

- âœ… Thu tháº­p vÃ  gÃ¡n nhÃ£n dá»¯ liá»‡u (831 áº£nh)
- âœ… Thiáº¿t láº­p hÆ°á»›ng dáº«n gÃ¡n nhÃ£n dá»¯ liá»‡u
- âœ… Thiáº¿t káº¿ phÆ°Æ¡ng phÃ¡p phÃ¢n táº§ng dá»¯ liá»‡u
- âœ… HoÃ n thÃ nh EDA vÃ  phÃ¢n tÃ­ch dataset
- ğŸŸ¡ Module 1 (Detection) - Äang phÃ¡t triá»ƒn
- ğŸŸ¡ Module 3 (Localization) - Äang phÃ¡t triá»ƒn

### Má»‘c Sáº¯p tá»›i

- [ ] HoÃ n thÃ nh training vÃ  Ä‘Ã¡nh giÃ¡ Module 1
- [ ] HoÃ n thÃ nh training vÃ  Ä‘Ã¡nh giÃ¡ Module 3
- [ ] Triá»ƒn khai Module 2 (ÄÃ¡nh giÃ¡ Cháº¥t lÆ°á»£ng)
- [ ] Triá»ƒn khai Module 4 (Náº¯n chá»‰nh Phá»‘i cáº£nh)
- [ ] Triá»ƒn khai Module 5 (TrÃ­ch xuáº¥t OCR)
- [ ] TÃ­ch há»£p vÃ  testing pipeline end-to-end
- [ ] Tá»‘i Æ°u hiá»‡u nÄƒng vÃ  benchmarking
- [ ] Chuáº©n bá»‹ triá»ƒn khai production
- [ ] TÃ­ch há»£p dá»‹ch vá»¥ backend

### Cáº£i tiáº¿n TÆ°Æ¡ng lai

- Xá»­ lÃ½ video real-time
- PhÃ¡t hiá»‡n nhiá»u container trong má»™t áº£nh
- Triá»ƒn khai mobile (TensorFlow Lite / ONNX)
- Dá»‹ch vá»¥ API vá»›i FastAPI
- Pipeline há»c liÃªn tá»¥c

---

## ÄÃ³ng gÃ³p

### CÃ¡ch ÄÃ³ng gÃ³p

ChÃºng tÃ´i hoan nghÃªnh má»i Ä‘Ã³ng gÃ³p! CÃ¡ch báº¡n cÃ³ thá»ƒ giÃºp Ä‘á»¡:

1. **BÃ¡o cÃ¡o Issues**: Sá»­ dá»¥ng GitHub Issues cho bÃ¡o cÃ¡o lá»—i vÃ  yÃªu cáº§u tÃ­nh nÄƒng
2. **Submit Pull Requests**: TuÃ¢n theo quy trÃ¬nh PR nÃªu trÃªn
3. **Cáº£i thiá»‡n TÃ i liá»‡u**: Sá»­a lá»—i chÃ­nh táº£, thÃªm vÃ­ dá»¥, lÃ m rÃµ giáº£i thÃ­ch
4. **Chia sáº» Ã tÆ°á»Ÿng**: Tháº£o luáº­n cáº£i tiáº¿n trong GitHub Discussions

### LiÃªn há»‡

- **TrÆ°á»Ÿng dá»± Ã¡n**: duyhxm
- **Tá»• chá»©c**: CÃ´ng ty SOWATCO
- **Email**: _(ThÃªm náº¿u cÃ³)_

---

## Giáº¥y phÃ©p & Ghi nháº­n

### Giáº¥y phÃ©p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo **Giáº¥y phÃ©p MIT**. Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t chi tiáº¿t.

### Ghi nháº­n

- **CÃ´ng ty SOWATCO** cho tÃ i trá»£ dá»± Ã¡n vÃ  chuyÃªn mÃ´n lÄ©nh vá»±c
- **Ultralytics** cho framework YOLOv11
- **DVC Team** cho cÃ´ng cá»¥ quáº£n lÃ½ phiÃªn báº£n dá»¯ liá»‡u
- **Weights & Biases** cho ná»n táº£ng theo dÃµi thá»±c nghiá»‡m
- **CVAT** cho cÃ´ng cá»¥ gÃ¡n nhÃ£n
- Cá»™ng Ä‘á»“ng mÃ£ nguá»“n má»Ÿ cho cÃ¡c thÆ° viá»‡n: OpenCV, PyTorch, Albumentations

### TÃ i liá»‡u Tham kháº£o

- [TÃ i liá»‡u YOLOv11](https://docs.ultralytics.com/)
- [TÃ i liá»‡u DVC](https://dvc.org/doc)
- [TÃ i liá»‡u Weights & Biases](https://docs.wandb.ai/)
- [Äá»‹nh dáº¡ng COCO Dataset](https://cocodataset.org/#format-data)

---

**Made with â¤ï¸ for SOWATCO**
