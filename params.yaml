# DVC Parameters Configuration
# Container ID Extraction Research Project
# Centralized configuration for all pipeline stages and model training

# ============================================================================
# DATA SPLITTING PARAMETERS
# ============================================================================
split:
  # Random seed for reproducibility
  seed: 42
  
  # Split ratios (must sum to 1.0)
  train_ratio: 0.70
  val_ratio: 0.15
  test_ratio: 0.15
  
  # Minimum number of instances to avoid singleton handling
  # Groups with fewer instances will trigger augmentation
  min_instances: 2

# ============================================================================
# STRATIFICATION PARAMETERS
# ============================================================================
stratification:
  # Environment factors (hard category)
  # Images with any of these attributes are prioritized
  r_env:
    - bad_light
    - occluded
    - not_clean
  
  # Geometric/sensor factors (tricky category)
  # Images with these attributes (but no r_env) are secondary priority
  r_geo:
    - frontal
    - blurry
  
  # Stratification groups
  groups:
    hard: r_env_present
    tricky: r_geo_present_only
    common: neither

# ============================================================================
# AUGMENTATION PARAMETERS (for singleton handling)
# ============================================================================
augmentation:
  # Augmentation strategy for rare samples
  # NOTE: NO horizontal flip for text/ID data (creates mirrored text)
  strategies:
    - shift_scale_rotate
    - random_brightness_contrast
    - motion_blur
  
  # Albumentations parameters
  shift_scale_rotate:
    shift_limit: 0.05
    scale_limit: 0.1
    rotate_limit: 5  # Small rotation only
    p: 0.7
  
  random_brightness_contrast:
    brightness_limit: 0.1
    contrast_limit: 0.1
    p: 0.5
  
  motion_blur:
    blur_limit: 3
    p: 0.3
  
  # Output naming convention
  prefix: aug_

# ============================================================================
# MODULE 1: CONTAINER DOOR DETECTION
# ============================================================================
detection:
  # COCO category information
  category_name: container_door
  category_id: 1
  
  # Image preprocessing
  img_size: 640  # YOLO input size
  
  # Model configuration
  model:
    architecture: yolov11n  # Options: yolov11n, yolov11s, yolov11m
    pretrained: true
    
  # Training hyperparameters
  training:
    epochs: 100
    batch_size: 16
    optimizer: AdamW
    learning_rate: 0.001
    weight_decay: 0.0005
    warmup_epochs: 3
    
    # Learning rate schedule
    lr_scheduler: cosine
    
    # Early stopping
    patience: 20
    
  # Augmentation (YOLO built-in)
  augmentation:
    hsv_h: 0.015
    hsv_s: 0.7
    hsv_v: 0.4
    degrees: 10.0
    translate: 0.1
    scale: 0.5
    shear: 10.0  # Enable shear for angled perspective (87% of dataset)
    perspective: 0.0
    flipud: 0.0
    fliplr: 0.5  # OK for door detection (symmetric)
    mosaic: 1.0
    mixup: 0.0
    copy_paste: 0.0
    
  # Validation
  validation:
    conf_threshold: 0.25
    iou_threshold: 0.45
    
  # Experiment tracking
  wandb:
    project: container-id-research
    entity: null  # Set to your wandb username
    name: detection_exp001_baseline
    tags:
      - module1
      - detection
      - yolov11n

# ============================================================================
# MODULE 2: IMAGE QUALITY ASSESSMENT (Future)
# ============================================================================
quality:
  # Quality features to assess
  features:
    - blur_score
    - brightness_score
    - contrast_score
    - size_score
    
  # Thresholds
  thresholds:
    blur_laplacian_min: 100
    brightness_min: 50
    brightness_max: 200
    size_min_pixels: 100000
    
  # Model (if using classifier)
  model:
    architecture: resnet18
    pretrained: true

# ============================================================================
# MODULE 3: CONTAINER ID LOCALIZATION
# ============================================================================
localization:
  # COCO category information
  category_name: container_id
  category_id: 2
  
  # Keypoints configuration
  num_keypoints: 4  # 4-point polygon (TL, TR, BR, BL)
  keypoint_names:
    - top_left
    - top_right
    - bottom_right
    - bottom_left
  
  # Filter strategy
  filter_unreadable: true  # Remove unreadable samples from training
  
  # Image preprocessing
  img_size: 640
  
  # Model configuration
  model:
    architecture: yolov11n-pose
    pretrained: true
    
  # Training hyperparameters
  training:
    epochs: 150
    batch_size: 16
    optimizer: AdamW
    learning_rate: 0.001
    weight_decay: 0.0005
    warmup_epochs: 3
    lr_scheduler: cosine
    patience: 30
    
  # Pose-specific parameters
  pose:
    kpt_shape: [4, 3]  # [num_keypoints, (x, y, visibility)]
    
  # Augmentation
  augmentation:
    hsv_h: 0.015
    hsv_s: 0.7
    hsv_v: 0.4
    degrees: 5.0  # Less rotation for pose
    translate: 0.1
    scale: 0.3
    shear: 0.0
    perspective: 0.0
    flipud: 0.0
    fliplr: 0.0  # DISABLED - No flip for text/ID (creates mirrored text)
    
  # Validation
  validation:
    conf_threshold: 0.25
    iou_threshold: 0.45
    kpt_threshold: 0.5
    
  # Experiment tracking
  wandb:
    project: container-id-research
    entity: null
    name: localization_exp001_baseline
    tags:
      - module3
      - localization
      - pose
      - yolov11

# ============================================================================
# MODULE 4: PERSPECTIVE CORRECTION (Future)
# ============================================================================
alignment:
  # Perspective transform parameters
  output_width: 400
  output_height: 100
  
  # Quality check after warping
  quality_check:
    enabled: true
    min_blur_score: 50

# ============================================================================
# MODULE 5: OCR EXTRACTION (Future)
# ============================================================================
ocr:
  # OCR engine selection
  engine: paddleocr  # Options: paddleocr, easyocr, tesseract
  
  # PaddleOCR configuration
  paddleocr:
    lang: en
    use_angle_cls: true
    use_gpu: true
    
  # Post-processing
  postprocess:
    # Container ID format validation
    # Expected: 4 letters + 6 digits + 1 check digit
    regex_pattern: "^[A-Z]{4}[0-9]{6}[0-9]$"
    
    # Character corrections (common OCR mistakes)
    char_corrections:
      "0": "O"  # Zero to O in letter positions
      "O": "0"  # O to zero in digit positions
      "1": "I"  # One to I in letter positions
      "I": "1"  # I to one in digit positions

# ============================================================================
# PIPELINE ORCHESTRATION
# ============================================================================
pipeline:
  # Module execution sequence
  modules:
    - detection
    - quality
    - localization
    - alignment
    - ocr
  
  # Confidence thresholds for pipeline flow
  thresholds:
    detection_conf: 0.5
    quality_gate_pass: 0.7
    localization_conf: 0.5
    alignment_quality: 0.6
    ocr_conf: 0.8
  
  # Output configuration
  output:
    save_intermediate: true
    save_visualizations: true
    format: json

# ============================================================================
# EXPERIMENT TRACKING (Global)
# ============================================================================
wandb:
  # Global wandb configuration
  project: container-id-research
  entity: null  # Set to your wandb username or team
  
  # Logging
  log_interval: 10  # Log every N batches
  log_images: 20    # Number of sample images to log
  
  # Artifact tracking
  save_code: true
  save_model: true

# ============================================================================
# HARDWARE & PERFORMANCE
# ============================================================================
hardware:
  # Device configuration
  device: cuda  # Options: cuda, cpu, mps (for Mac M1/M2)
  
  # Multi-GPU training
  multi_gpu: false
  gpu_ids: [0]
  
  # Performance
  num_workers: 4  # DataLoader workers
  pin_memory: true
  mixed_precision: true  # AMP for faster training

# ============================================================================
# PATHS (Relative to project root)
# ============================================================================
paths:
  # Data directories
  raw_data: data/raw
  annotations: data/annotations
  interim_data: data/interim
  processed_data: data/processed
  
  # Output directories
  weights: weights
  experiments: experiments
  logs: logs
  
  # Specific dataset paths
  detection_dataset: data/processed/detection
  localization_dataset: data/processed/localization
  quality_dataset: data/processed/quality
  ocr_dataset: data/processed/ocr

